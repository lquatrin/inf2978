import timeimport mathimport randomimport bwreadersimport bwdistanceimport bwnumbergenimport bwserializationimport bwprojectionimport bwmathimport numpy as npimport osimport gcimport bwrunsimport svddocsprojectionmy_out_path = ''# QtTecgraf#my_out_path = 'C:/Users/lquatrin/Dropbox/data/'# QtHome#my_out_path = 'E:/Dropbox/data/home/'output_path = my_out_pathDOCWORD_FILE = "../docword.nytimes.txt"VOCAB_FILE = "vocab.nytimes.txt"# 1 2# 1. Baixe o dataset Bag of Words da UCI (arquivo NyTimes). Cerca de 300k docs e vocabulario com 102650 termos# 2. Crie uma bag of words para os 3000 primeiros documentosword_list = bwreaders.ReadVocabulary(VOCAB_FILE)D_t_work = 3000table_docs = bwreaders.ReadDocuments(DOCWORD_FILE, D_t_work)#documents =  bwreaders.read_document_corpus(filepath= './docword.nytimes.txt_preprocessed.txt',#                                              num_docs=3000,#                                              num_words_in_vocabulary=102650)documents = table_docs[0]# n documentosD = table_docs[1]# n palavrasW = table_docs[2]# n entradasNNZ = table_docs[3]# 3# 3. Calcule a distancia entre cada par de pontos atraves da forca bruta e messa o tempo computacional deste procedimento. Armazene estes valores. Utilize dois loops para fazer isso e implemente o calculo da distanciamtx_original_distance = bwdistance.GenerateOriginalDistanceMatrix(D_t_work, W, documents)# Create DataDocs#data_doc = np.zeros(shape = (W, D_t_work))#for doc_id in range(D_t_work):#  for word_id, count_w in documents[doc_id].items():#    data_doc[word_id, doc_id] = float(count_w)#documents = data_doc# 4# 4. Para n = 4, 16, 64, 256, 1024, 4096, 15768, repita o procedimento abaixo 30 vezesrepeat = 30n_cases = [ ]#4, 16, 64, 256, 1024, 4096, 15768 ]for N_case in n_cases:  # 4.6  text_file = open(my_out_path + str(N_case) + "_JL.txt", "w")  text_file.write("%f" % (bwmath.CalculateJLLema(W, N_case)))  text_file.close()    ####################################################################  # Versão 1:  # - Geração da matriz   # - Projeção (maior consumo de memória)  ####################################################################  bwruns.RunVersion01(documents, mtx_original_distance, N_case, repeat, W, D_t_work, output_path, True, True)  ####################################################################  # Versão 2:  # - Geração e projeção ao mesmo tempo (menor consumo de memória)  ####################################################################  bwruns.RunVersion02(documents, mtx_original_distance, N_case, repeat, W, D_t_work, output_path, True, True)#https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf#https://pdfs.semanticscholar.org/4dd0/146ae1d403aba8d0774315f4e2386948034f.pdf#http://www.dt.fee.unicamp.br/~ivanil/adaptive_svd_encoder-analise_metodo_wei.pdf#http://www.geometrie.tugraz.at/sgp2015/slides/3a_Mapping_Poranne/Mappings_partA.pdf#http://bud.cs.uky.edu/~jzhang/pub/PRINT/xu6.pdf#https://en.wikipedia.org/wiki/Singular_value_decomposition#http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htmsvd_cases = [ 4, 16, 64, 256 ]for N_svd_case in svd_cases:  ####################################################################  # Versão 3:  # - SVD  ####################################################################  svddocsprojection.SVDDocsProjection(documents, mtx_original_distance, N_svd_case, W, D_t_work, output_path)